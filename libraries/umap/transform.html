
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  
  <!-- Licensed under the Apache 2.0 License -->
  <link rel="stylesheet" type="text/css" href="../../_static/fonts/open-sans/stylesheet.css" />
  <!-- Licensed under the SIL Open Font License -->
  <link rel="stylesheet" type="text/css" href="../../_static/fonts/source-serif-pro/source-serif-pro.css" />
  <link rel="stylesheet" type="text/css" href="../../_static/css/bootstrap.min.css" />
  <link rel="stylesheet" type="text/css" href="../../_static/css/bootstrap-theme.min.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
    <title>Transforming New Data with UMAP &#8212; scikit-tda 0.0.2 documentation</title>
    <link rel="stylesheet" href="../../_static/guzzle.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="UMAP for Supervised Dimension Reduction and Metric Learning" href="supervised.html" />
    <link rel="prev" title="Basic UMAP Parameters" href="parameters.html" />
  
   

  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="supervised.html" title="UMAP for Supervised Dimension Reduction and Metric Learning"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="parameters.html" title="Basic UMAP Parameters"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">scikit-tda 0.0.2 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../index.html" >Libraries</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="index.html" accesskey="U">UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</a> &#187;</li> 
      </ul>
    </div>
    <div class="container-wrapper">

      <div id="mobile-toggle">
        <a href="#"><span class="glyphicon glyphicon-align-justify" aria-hidden="true"></span></a>
      </div>
  <div id="left-column">
    <div class="sphinxsidebar">
<div class="sidebar-block">
  <div class="sidebar-wrapper">
    <h2>Table Of Contents</h2>
  </div>
  <div class="sidebar-toc">
    
    
      <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../about.html">About</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Libraries</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../kepler-mapper/index.html">KeplerMapper</a></li>
<li class="toctree-l2"><a class="reference internal" href="../persim/index.html">Persim</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#indices-and-tables">Indices and tables</a></li>
</ul>
</li>
</ul>

    
  </div>
</div>
  <h4>Previous topic</h4>
  <p class="topless"><a href="parameters.html"
                        title="previous chapter">Basic UMAP Parameters</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="supervised.html"
                        title="next chapter">UMAP for Supervised Dimension Reduction and Metric Learning</a></p>
<div class="sidebar-block">
  <div class="sidebar-wrapper">
    <div id="main-search">
      <form class="form-inline" action="../../search.html" method="GET" role="form">
        <div class="input-group">
          <input name="q" type="text" class="form-control" placeholder="Search...">
        </div>
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
      </form>
    </div>
  </div>
</div>
      
    </div>
  </div>
        <div id="right-column">
          
          <div role="navigation" aria-label="breadcrumbs navigation">
            <ol class="breadcrumb">
              <li><a href="../../index.html">Docs</a></li>
              
                <li><a href="../index.html">Libraries</a></li>
              
                <li><a href="index.html">UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</a></li>
              
              <li>Transforming New Data with UMAP</li>
            </ol>
          </div>
          
          <div class="document clearer body">
            
  <div class="section" id="transforming-new-data-with-umap">
<h1>Transforming New Data with UMAP<a class="headerlink" href="#transforming-new-data-with-umap" title="Permalink to this headline">¶</a></h1>
<p>UMAP is useful for generating visualisations, but if you want to make
use of UMAP more generally for machine learning tasks it is important to
be be able to train a model and then later pass new data to the model
and have it transform that data into the learned space. For example if
we use UMAP to learn a latent space and then train a classifier on data
transformed into the latent space then the classifier is only useful for
prediction if we can transform data for which we want a prediction into
the latent space the classifier uses. Fortunately UMAP makes this
possible, albeit more slowly than some other transformers that allow
this.</p>
<p>To demonstrate this functionality we’ll make use of
<a class="reference external" href="http://scikit-learn.org/stable/index.html">scikit-learn</a> and the
digits dataset contained therein (see <a class="reference internal" href="basic_usage.html"><span class="doc">How to Use UMAP</span></a> for an example
of the digits dataset). First let’s load all the modules we’ll need to
get this done.</p>
<div class="code python3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_digits</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="k">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="k">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="k">import</span> <span class="n">SVC</span>

<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</pre></div>
</div>
<div class="code python3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">context</span><span class="o">=</span><span class="s1">&#39;notebook&#39;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">rc</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;figure.figsize&#39;</span><span class="p">:(</span><span class="mi">14</span><span class="p">,</span><span class="mi">10</span><span class="p">)})</span>
</pre></div>
</div>
<div class="code python3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>
</pre></div>
</div>
<p>To keep everything honest let’s use sklearn <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> to
separate out a training and test set (stratified over the different
digit types). By default <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> will carve off 25% of the
data for testing, which seems suitable in this case.</p>
<div class="code python3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
                                                    <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
                                                    <span class="n">stratify</span><span class="o">=</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
<p>Now to get a benchmark idea of what we are looking at let’s train a
couple of different classifiers and then see how well they score on the
test set. For this example lets try a support vector classifier and a
KNN classifier. Ideally we should be tuning hyper-parameters (perhaps a
grid search using k-fold cross validation), but for the purposes of this
simple demo we will simply use default parameters for both classifiers.</p>
<div class="code python3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">svc</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<p>The next question is how well these classifiers perform on the test set.
Conveniently sklearn provides a <code class="docutils literal notranslate"><span class="pre">score</span></code> method that can output the
accuracy on the tets set.</p>
<div class="code python3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mf">0.62</span><span class="p">,</span> <span class="mf">0.9844444444444445</span><span class="p">)</span>
</pre></div>
</div>
<p>The result is that the support vector classifier apparently had poor
hyper-parameters for this case (I expect with some tuning we could build
a much more accurate mode) and the KNN classifier is doing very well.</p>
<p>The goal now is to make use of UMAP as a preprocessing step that one
could potentially fit into a pipeline. We will therefore obviously need
the <code class="docutils literal notranslate"><span class="pre">umap</span></code> module loaded.</p>
<div class="code python3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">umap</span>
</pre></div>
</div>
<p>To make use of UMAP as a data transformer we first need to fir the model
with the training data. This works exactly as in the <a class="reference internal" href="basic_usage.html"><span class="doc">How to Use UMAP</span></a>
example using the fit method. In this case we simply hand it the
training data and it will learn an appropriate (two dimensional by
default) embedding.</p>
<div class="code python3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">trans</span> <span class="o">=</span> <span class="n">umap</span><span class="o">.</span><span class="n">UMAP</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>
</div>
<p>Since we embedded to two dimensions we can visualise the results to
ensure that we are getting a potential benefit out of this approach.
This is simply a matter of generating a scatterplot with data points
colored by the class they come from. Note that the embedded training
data can be accessed as the <code class="docutils literal notranslate"><span class="pre">.embedding_</span></code> attribute of the UMAP model
once we have fit the model to some data.</p>
<div class="code python3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">trans</span><span class="o">.</span><span class="n">embedding_</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">trans</span><span class="o">.</span><span class="n">embedding_</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Spectral&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Embedding of the training set by UMAP&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">);</span>
</pre></div>
</div>
<img alt="../../_images/UMAPTransform_15_0.png" src="../../_images/UMAPTransform_15_0.png" />
<p>This looks very promising! Most of the classes got very cleanly
separated, and that gives us some hope that it could help with
classifier performance. It is worth noting that this was a completely
unsupervised data transform; we could have used the training label
information, but that is the subject of <a class="reference internal" href="supervised.html"><span class="doc">a later tutorial</span></a>.</p>
<p>We can now train some new models (again an SVC and a KNN classifier) on
the embedded training data. This looks exactly as before but now we pass
it the embedded data. Note that calling <code class="docutils literal notranslate"><span class="pre">transform</span></code> on input identical
to what the model was trained on will simply return the <code class="docutils literal notranslate"><span class="pre">embedding_</span></code>
attribute, so sklearn pipelines will work as expected.</p>
<div class="code python3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">svc</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trans</span><span class="o">.</span><span class="n">embedding_</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trans</span><span class="o">.</span><span class="n">embedding_</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we want to work with the test data which none of the models (UMAP or
the classifiers) have seen. To do this we use the standard sklearn API
and make use of the <code class="docutils literal notranslate"><span class="pre">transform</span></code> method, this time handing it the new
unseen test data. We will assign this to <code class="docutils literal notranslate"><span class="pre">test_embedding</span></code> so that we
can take a closer look at the result of applying an existing UMAP model
to new data.</p>
<div class="code python3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="n">time</span> <span class="n">test_embedding</span> <span class="o">=</span> <span class="n">trans</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">CPU</span> <span class="n">times</span><span class="p">:</span> <span class="n">user</span> <span class="mi">867</span> <span class="n">ms</span><span class="p">,</span> <span class="n">sys</span><span class="p">:</span> <span class="mf">70.7</span> <span class="n">ms</span><span class="p">,</span> <span class="n">total</span><span class="p">:</span> <span class="mi">938</span> <span class="n">ms</span>
<span class="n">Wall</span> <span class="n">time</span><span class="p">:</span> <span class="mi">335</span> <span class="n">ms</span>
</pre></div>
</div>
<p>Note that the transform operations works very efficiently – taking less
than half a second. Compared to some other transformers this is a little
on the slow side, but it is fast enough for many uses. Note that as the
size of the training and/or test sets increase the performance will slow
proportionally. It’s also worth noting that the first call to transform
may be slow due to Numba JIT overhead – further runs will be very fast.</p>
<p>The next important question is what the transform did to our test data.
In principle we have a new two dimensional representation of the
test-set, and ideally this should be based on the existing embedding of
the training set. We can check this by visualising the data (since we
are in two dimensions) to see if this is true. A simple scatterplot as
before will suffice.</p>
<div class="code python3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">test_embedding</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">test_embedding</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">s</span><span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">y_test</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;Spectral&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Embedding of the test set by UMAP&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">24</span><span class="p">);</span>
</pre></div>
</div>
<img alt="../../_images/UMAPTransform_21_0.png" src="../../_images/UMAPTransform_21_0.png" />
<p>The results look like what we should expect; the test data has been
embedded into two dimensions in exactly the locations we should expect
(by class) given the embedding of the training data visualised above.
This means we can now try out of models that were trained on the
embedded training data by handing them the newly transformed test set.</p>
<div class="code python3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">trans</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">y_test</span><span class="p">),</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">trans</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mf">0.9844444444444445</span><span class="p">,</span> <span class="mf">0.9844444444444445</span><span class="p">)</span>
</pre></div>
</div>
<p>The results are pretty good. While the accuracy of the KNN classifier
did not improve there was not a lot of scope for improvement given the
data. On the other hand the SVC has improved to have equal accuracy to
the KNN classifier. Of course we could probably have achieved this level
of accuracy by better setting SVC hyper-parameters, but the point here
is that we can use UMAP as if it were a standard sklearn transformer as
part of an sklearn machine learning pipeline.</p>
<p>Just for fun we can run the same experiments, but this time reduce to
ten dimensions (where we can no longer visualise). In practice this will
have little gain in this case – for the digits dataset two dimensions
is plenty for UMAP and more dimensions won’t help. On the other had for
more complex datasets where more dimensions may allow for a much more
faithful embedding it is worth noting that we are not restricted to only
two dimension.</p>
<div class="code python3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">trans</span> <span class="o">=</span> <span class="n">umap</span><span class="o">.</span><span class="n">UMAP</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_components</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
</pre></div>
</div>
<div class="code python3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">svc</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trans</span><span class="o">.</span><span class="n">embedding_</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">trans</span><span class="o">.</span><span class="n">embedding_</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<div class="code python3 highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">svc</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">trans</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">y_test</span><span class="p">),</span> <span class="n">knn</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">trans</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="mf">0.9822222222222222</span><span class="p">,</span> <span class="mf">0.9822222222222222</span><span class="p">)</span>
</pre></div>
</div>
<p>And we see that in this case we actually marginally lowered our accuracy
scores (within the potential noise in such scoring mind you). However
for more interesting datasets the larger dimensional embedding may have
been a significant gain – it is certainly worth exploring as one of the
parameters in a grid search across a pipeline that includes UMAP.</p>
</div>


          </div>
            
  <div class="footer-relations">
    
      <div class="pull-left">
        <a class="btn btn-default" href="parameters.html" title="previous chapter (use the left arrow)">Basic UMAP Parameters</a>
      </div>
    
      <div class="pull-right">
        <a class="btn btn-default" href="supervised.html" title="next chapter (use the right arrow)">UMAP for Supervised Dimension Reduction and Metric Learning</a>
      </div>
    </div>
    <div class="clearer"></div>
  
        </div>
        <div class="clearfix"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="supervised.html" title="UMAP for Supervised Dimension Reduction and Metric Learning"
             >next</a> |</li>
        <li class="right" >
          <a href="parameters.html" title="Basic UMAP Parameters"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">scikit-tda 0.0.2 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../index.html" >Libraries</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="index.html" >UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction</a> &#187;</li> 
      </ul>
    </div>
<script type="text/javascript">
  $("#mobile-toggle a").click(function () {
    $("#left-column").toggle();
  });
</script>
<script type="text/javascript" src="../../_static/js/bootstrap.js"></script>
  <div class="footer">
    &copy; Copyright 2018, Nathaniel Saul. Created using <a href="http://sphinx.pocoo.org/">Sphinx</a>.
  </div>
  </body>
</html>